# -*- coding: utf-8 -*-
"""FR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zl2WpR8wfiNUApXq6IMWgdi8llbEdvZQ
"""

import numpy as np

import pandas as pd

df=pd.read_csv('/content/data.csv')
df



df.head(2)

df.tail(2)

df.info

df.describe()

df.shape

df.info()

df.isna().sum()

import pandas as pd
import matplotlib.pyplot as plt
import cufflinks as cf
import plotly
import plotly.express as px
import seaborn as sns

from IPython.core.display import HTML
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from datetime import datetime
from sklearn.feature_extraction.text import CountVectorizer
from pandas import DataFrame
from collections import OrderedDict

import pandas as pd
import matplotlib.pyplot as plt

# ... (rest of your imports)

# Assuming your DataFrame is named 'df'

# Print the columns of your DataFrame to identify the correct column name
print(df.columns)

# Replace 'type' with the actual column name (e.g., 'article_type')
# Assuming the correct column name is 'article_type'
df['Label'].value_counts().plot.pie(figsize=(8, 8), startangle=75)
plt.title('Types of articles', fontsize=20)
plt.axis('off')
plt.show()

from tqdm import tqdm
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud

def preprocess_text(text_data):
    preprocessed_text = []

    for sentence in tqdm(text_data):
        sentence = re.sub(r'[^\w\s]', '', sentence)
        preprocessed_text.append(' '.join(token.lower()
                                  for token in str(sentence).split()
                                  if token not in stopwords.words('english')))

    return preprocessed_text

from tqdm import tqdm
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud


def preprocess_text(text_data):
    preprocessed_text = []

    for sentence in tqdm(text_data):
        # Convert sentence to string before applying regex
        sentence = str(sentence)
        sentence = re.sub(r'[^\w\s]', '', sentence)
        preprocessed_text.append(' '.join(token.lower()
                                  for token in str(sentence).split()
                                  if token not in stopwords.words('english')))

    return preprocessed_text

consolidated = ' '.join(
    word for word in df['Body'][df['Label'] == 1].astype(str))
wordCloud = WordCloud(width=1600,
                      height=800,
                      random_state=21,
                      max_font_size=110,
                      collocations=False)
plt.figure(figsize=(15, 10))
plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')
plt.axis('off')
plt.show()

consolidated = ' '.join(
    word for word in df['Headline'][df['Label'] == 1].astype(str) if word != '')

# Explanation of changes:
# 1. Changed the condition from df['Body'] == 0 to df['Label'] == 1
#    to select rows with Label 1 instead of filtering by Body column
#    This is based on assumption that 'Label' is your target variable.
# 2. Keeping the filter `if word != ''` to remove empty strings,
#    ensuring only valid words are included

wordCloud = WordCloud(width=1600,
                      height=800,
                      random_state=21,
                      max_font_size=110,
                      collocations=False)
plt.figure(figsize=(15, 10))
plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')
plt.axis('off')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer


def get_top_n_words(corpus, n=None):
    vec = CountVectorizer().fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx])
                  for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1],
                        reverse=True)
    return words_freq[:n]


common_words = get_top_n_words(df['Headline'], 20)
df1 = pd.DataFrame(common_words, columns=['Review', 'count'])

df1.groupby('Review').sum()['count'].sort_values(ascending=False).plot(
    kind='bar',
    figsize=(10, 6),
    xlabel="Top Words",
    ylabel="Count",
    title="Bar Chart of Top Words Frequency"
)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer


# 1. Create a TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# 2. Fit the vectorizer to your training data and transform it
x_train_vec = vectorizer.fit_transform(df['Headline'])

# 3. Split the data into training and testing sets
# Use the original 'Headline' column for x values in train_test_split
x_train, x_test, y_train, y_test = train_test_split(df['Headline'],
                                                    df['Label'],
                                                    test_size=0.25)

# 4. Transform the training and testing data using the fitted vectorizer
x_train_vec = vectorizer.transform(x_train)
x_test_vec = vectorizer.transform(x_test)

# 5. Create and train the model
model = LogisticRegression()
model.fit(x_train_vec, y_train)  # Use transformed training data


# 6. Test the model and print accuracy
print(accuracy_score(y_train, model.predict(x_train_vec)))  # Use transformed training data
print(accuracy_score(y_test, model.predict(x_test_vec)))    # Use transformed test data

from sklearn import metrics

# Use the transformed test data (x_test_vec) instead of original text data (x_test)
cm = metrics.confusion_matrix(y_test, model.predict(x_test_vec))

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,
                                            display_labels=[False, True])

cm_display.plot()
plt.show()

"""# Decision Tree"""

import pandas as pd
from sklearn.tree import DecisionTreeClassifier

dt=DecisionTreeClassifier(criterion='entropy')
#fit the model
# Use the transformed training data (x_train_vec, y_train) for DecisionTreeClassifier
dt.fit(x_train_vec, y_train)
#predict the model
# Use transformed test data for prediction
y_pred_test=dt.predict(x_test_vec)
# Use transformed train data for prediction
y_pred_train=dt.predict(x_train_vec)

accuracy_train=accuracy_score(y_train,y_pred_train) # Replace train_y with y_train
print("accuracy of the deision tree model on your trai dataset is:",accuracy_train)

accuracy_test=accuracy_score(y_train,y_pred_train)
print("accuracy of the decision tree model on your test dataset is:",accuracy_test)

"""# Logistic Regression"""

from sklearn.model_selection import train_test_split

train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.2,random_state=2)

print(train_x.head())
print(train_y.head())

from sklearn.linear_model import LogisticRegression

#call the model
model_lr=LogisticRegression(max_iter=1000)

#train/fit the model using the transformed data (X_train, Y_train)
model_lr.fit(X_train, Y_train)

#predict from the model using the transformed data (X_test)
y_pred_train = model_lr.predict(X_train)
y_pred_test = model_lr.predict(X_test)

print(y_pred_train)
print(y_pred_test)

"""# Convolution Neural Network"""

# Import necessary libraries
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Step 1: Load and preprocess the dataset
# Replace 'your_dataset.csv' with your actual dataset file path
df = pd.read_csv('your_dataset.csv')

# Display the first few rows of the dataset for verification
print(df.head())

# Ensure the dataset has the correct columns
# Replace 'text_column' and 'label_column' with your actual column names
df = df[['text_column', 'label_column']].dropna()  # Drop missing values
df['label_column'] = df['label_column'].map({'real': 0, 'fake': 1})  # Map labels to binary

# Extract features and labels
texts = df['text_column'].values  # Replace 'text_column' with the actual column name
labels = df['label_column'].values

# Step 2: Tokenize and pad text data
max_words = 5000  # Vocabulary size
max_len = 100  # Maximum sequence length

tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')

# Step 3: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Step 4: Build the CNN model
model = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),  # Embedding layer
    Conv1D(128, kernel_size=5, activation='relu'),  # Convolutional layer
    GlobalMaxPooling1D(),  # Max pooling
    Dense(64, activation='relu'),  # Fully connected layer
    Dropout(0.5),  # Dropout for regularization
    Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Step 5: Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Step 6: Train the model
history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)

# Step 7: Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Step 8: Make predictions and evaluate the model
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Import necessary libraries
import pandas as pd
   #... (rest of your imports)

   # Step 1: Load and preprocess the dataset
   # Replace 'your_dataset.csv' with your actual dataset file path or upload the file
   # and provide the correct path here
   df = pd.read_csv('/content/data.csv')  # Update with your actual file path

   # Display the first few rows of the dataset for verification
   print(df.head())

   #... (rest of your code)
   df = df[['text_column', 'label_column']].dropna()  # Drop missing values
df['label_column'] = df['label_column'].map({'real': 0, 'fake': 1})  # Map labels to binary

# Extract features and labels
texts = df['text_column'].values  # Replace 'text_column' with the actual column name
labels = df['label_column'].values

# Step 2: Tokenize and pad text data
max_words = 5000  # Vocabulary size
max_len = 100  # Maximum sequence length

tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')

# Step 3: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Step 4: Build the CNN model
model = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),  # Embedding layer
    Conv1D(128, kernel_size=5, activation='relu'),  # Convolutional layer
    GlobalMaxPooling1D(),  # Max pooling
    Dense(64, activation='relu'),  # Fully connected layer
    Dropout(0.5),  # Dropout for regularization
    Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Step 5: Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Step 6: Train the model
history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)

# Step 7: Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Step 8: Make predictions and evaluate the model
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

!pip install tensorflow
!pip install pandas
!pip install nltk
!pip install scikit-learn


import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import nltk
nltk.download('punkt')
nltk.download('stopwords')


# Step 1: Load and preprocess the dataset
df = pd.read_csv('/content/data.csv')  # Update with your actual file path

# Assuming your columns are 'Headline' and 'Label'
df = df[['Headline', 'Label']].dropna()  # Drop missing values if any
df['Label'] = df['Label'].astype(int)  # Convert Label to numeric

# Display the first few rows for verification
print(df.head())


# Extract features and labels
texts = df['Headline'].values
labels = df['Label'].values

# Step 2: Tokenize and pad text data
max_words = 5000  # Vocabulary size
max_len = 100  # Maximum sequence length

tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')

# Step 3: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Step 4: Build the CNN model
model = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),
    Conv1D(128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Step 5: Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Step 6: Train the model
history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)

# Step 7: Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Step 8: Make predictions and evaluate the model
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))